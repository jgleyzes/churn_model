{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import plotly.graph_objects as go\n",
    "import preprocessing\n",
    "import metrics\n",
    "import plotting_utils\n",
    "from hyperparameter_tuner import LinearSVCHyperParameterTuner, KNNHyperParameterTuner, LGBMHyperParameterTuner\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaebb6e",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acab381",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaccf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"data/Churn_Modelling.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bbbd7",
   "metadata": {},
   "source": [
    "Prepare data (remove uninformative columns and convert string to one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8474a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing.drop_columns_and_convert_strings(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a246916",
   "metadata": {},
   "source": [
    "Look at the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeac3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_correlations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743d189",
   "metadata": {},
   "source": [
    "Look at pairplot (quite slow, so loading precomputed one by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83316483",
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed = True\n",
    "if not precomputed:\n",
    "    sns_plot = sns.pairplot(df, hue=\"Exited\")\n",
    "    sns_plot.figure.savefig(\"pairplot.jpg\")\n",
    "Image.open(\"pairplot.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7b0f5",
   "metadata": {},
   "source": [
    "From these two plots, it seems Age (high correlation) and NumOfProducts (high NumOfProducts are almost always Exited in the pairplot) are important features. We will see if this is confirmed later in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403f1ff",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, stratify=df[\"Exited\"], random_state=2)\n",
    "X_train, y_train = preprocessing.split_label(df_train)\n",
    "X_test, y_test = preprocessing.split_label(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c156577",
   "metadata": {},
   "source": [
    "To guide our exploration, we will use two metrics:\n",
    "1) Precision at given recall. This one is more business oriented. Since we are predicting churn, we want to have a high recall (we don't want to miss customers that are likely to exit). We could choose with the concerned product manager an acceptable rate of recall (I will assume 90% in the following). The metric we will optimize is therefore the precision corresponding to that recall. Higher precision means fewer False Positive. This approach assumes the procedure when a customer is predicted as churn is not too high (there will be a high number of false positive). It seems reasonable for the case where we would just send an email to that customer.\n",
    "\n",
    "2) The area under the curve of the Receiver Operating Characteristic (roc_auc). This is a more generic metric, but not uncorrelated with the first one (higher roc_auc will mean higher precision). It's one that's more familiar to ML engineers and therefore could speak more to their intuition.\n",
    "\n",
    "Precision at 90% recall is the metric we will optimize, while still keeping an eye on roc_auc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d28a1",
   "metadata": {},
   "source": [
    "### Random classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679b940",
   "metadata": {},
   "source": [
    "To get an idea of whether ML actually makes an improvement, we'll see what type of scores we get from a random classifier (it assigns a random score between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9def76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_classifier import RandomClassifier\n",
    "       \n",
    "random_model = RandomClassifier()\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, random_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbae77a",
   "metadata": {},
   "source": [
    "### Linear SVC model (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4b9f0",
   "metadata": {},
   "source": [
    "As a baseline, we will use a simple linear svc model. We'll gradually complexify the models (and/or preprocessing) hoping to improve on this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "linear_svc = Pipeline([('scaler', MinMaxScaler()), ('clf', LinearSVC(dual=False))])\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, linear_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d3e6c",
   "metadata": {},
   "source": [
    "It seems we are doing much better than the random classifier (35% increase in precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = linear_svc.named_steps.clf.coef_[0]\n",
    "plotting_utils.plot_importance(coefs, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29c23b",
   "metadata": {},
   "source": [
    "We find that Age is indeed an important feature as expected from the correlation analysis. IsActiveMember is also quite important, with a negative correlation. This makes sense, as active member should be less inclined to churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7e5e7",
   "metadata": {},
   "source": [
    "### Tuned Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa337a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_tuner = LinearSVCHyperParameterTuner()\n",
    "tuned_linear_svc = linear_svc_tuner.get_tuned_model(df_train, 20)\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, tuned_linear_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b526a16",
   "metadata": {},
   "source": [
    "The tuned model doesn't do significantly better. Let's move on to more complex algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af21db7",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde2f43",
   "metadata": {},
   "source": [
    "KNN seems like a good model for churn: if a customer A churns, we can guess that a customer B similar to A is likely to churn as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18886fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = Pipeline([('scaler', MinMaxScaler()), ('clf', KNeighborsClassifier())])\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aba81",
   "metadata": {},
   "source": [
    "The choice of n_neighbors in KNeighborsClassifier is highly data dependent. To find the optimal one we will run a 1D simple hyperparameter optimization using Optuna (not really necessary here, but since we will use it for later models, we might as well use it here also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tuner = KNNHyperParameterTuner()\n",
    "tuned_knn = knn_tuner.get_tuned_model(df_train, 2)\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, tuned_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a04f20",
   "metadata": {},
   "source": [
    "It's not necessarily better. However, drawing conclusion from a single test set is dangerous. We'll do a more careful model comparison later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf004e6",
   "metadata": {},
   "source": [
    "### Base LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3de64",
   "metadata": {},
   "source": [
    "Gradient boosted trees are strong models for tabular data. In particular, the lightgbm library provides a fast implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c23b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "base_lgbm = LGBMClassifier()\n",
    "base_lgbm.fit(X_train, y_train)\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, base_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a3ea8",
   "metadata": {},
   "source": [
    "This is quite an increase compared to our baseline {'precision_at_90.0_recall': 0.2718532794068828, 'roc_auc': 0.7594163914432234}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58230a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_importance(base_lgbm.feature_importances_, base_lgbm.feature_name_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89400e05",
   "metadata": {},
   "source": [
    "Age is again our top feature. Here we see that NumOfProducts is also high in the list, as expected from the pairplot analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b10fa",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415f803",
   "metadata": {},
   "source": [
    "Models such as boosted trees with provide feature importances can be use to remove less informative features.\n",
    "Implementation is based on sklearn.feature_selection.RFECV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6788cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recursive_feature_elimination import ModelRecursiveFeatureElimination\n",
    "rfeliminator = ModelRecursiveFeatureElimination()\n",
    "tree_classifier_rfe = rfeliminator.get_model(X_train, y_train, LGBMClassifier())\n",
    "tree_classifier_rfe.fit(X_train, y_train)\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, tree_classifier_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b627fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_importance(tree_classifier_rfe.named_steps.clf.feature_importances_, rfeliminator.cols_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c254cf",
   "metadata": {},
   "source": [
    "There doesn't seem to be any significant improvement from the RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f64383",
   "metadata": {},
   "source": [
    "### Tuned LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28b7b1",
   "metadata": {},
   "source": [
    "LGBM has a lot of hyperparameters, choosing the right ones can make a big difference. We're going to do some exploration with Optuna, optimizing for precision at 90% recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207632cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the tuning takes some time (typically 5min), can load trained model instead\n",
    "do_tuning = False\n",
    "if do_tuning:\n",
    "    lgbm_tuner = LGBMHyperParameterTuner()\n",
    "    tuned_lgbm = lgbm_tuner.get_tuned_model(df_train, n_trials=100)\n",
    "else:\n",
    "    tuned_lgbm = pickle.load(open(\"inference/model/tuned_lgbm.pickle\", \"rb\"))\n",
    "print(\"\\n On test set, we have\")\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, tuned_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_importance(tuned_lgbm.feature_importances_, tuned_lgbm.feature_name_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc37c44",
   "metadata": {},
   "source": [
    "This seems to be the best candidate so far. We'll see if a more robust analysis confirms it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03217711",
   "metadata": {},
   "source": [
    "## Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701251d",
   "metadata": {},
   "source": [
    "Another option is to combined the previous models into a Meta Model. Since we want to compute precision for a given recall, we need to have a score (ie a float), not just a prediction (in (0, 1)). In order to combine scores which comes for very different models, we will map the scores to the corresponding precision value, using an interpolated function computed on the training data to avoid data leakage. More details in the meta_model python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models = {\n",
    "    \"linear_svc\": linear_svc,\n",
    "    \"tuned_linear_svc\": tuned_linear_svc,\n",
    "    \"knn\": knn,\n",
    "    \"tuned_knn\": tuned_knn,\n",
    "    \"base_lgbm\": base_lgbm,\n",
    "    \"tree_classifier_rfe\": tree_classifier_rfe,\n",
    "    \"tuned_lgbm\": tuned_lgbm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_model import MetaModel\n",
    "\n",
    "meta_model = MetaModel(dict_models)\n",
    "meta_model.fit(X_train, y_train)\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, meta_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796abf9",
   "metadata": {},
   "source": [
    "This doesn't seem to be much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9655146",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fc6f8",
   "metadata": {},
   "source": [
    "Feature engineering can be a powerful tool to help machine learning models. However, nothing really jumps out from the pair plot I first did. In the absence of more information about the business to guide our intuition, we would need to resort to blind exploration. This can be very time consuming so I decided to focus on other aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f458dfb",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749e48f",
   "metadata": {},
   "source": [
    "To determine whether model A is significantly better than model B, it's not sufficient to compare the performance on a given test set. It could very well be that the uncertainty on the model performance (coming from the fact that the test set has a finite size) is larger than the difference in performance. In order to robustly assess which model is better, we need train and evaluate on many different train test splits. If a model is better than the others most of the time, then we can be more confident it will be better once in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e802efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_comparison import ModelComparator\n",
    "from copy import deepcopy\n",
    "new_dict_models = {\n",
    "   \"random_model\": random_model,\n",
    "   \"linear_svc\": linear_svc,\n",
    "   \"tuned_linear_svc\": tuned_linear_svc,\n",
    "   \"knn\": knn,\n",
    "   \"tuned_knn\": tuned_knn,\n",
    "    \"base_lgbm\": base_lgbm,\n",
    "   \"tree_classifier_rfe\": tree_classifier_rfe,\n",
    "    \"tuned_lgbm\": tuned_lgbm,\n",
    "   \"meta_model\": meta_model\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_precision_recall_multiples(X_test, y_test, new_dict_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f051ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparator = ModelComparator(new_dict_models, n_tries=100)\n",
    "precision_scores, auc_scores = model_comparator.compare_models(*preprocessing.split_label(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae9c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Histogram(x=precision_scores[col], nbinsx=30, name=col) for col in precision_scores if col != \"best\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Histogram(x=auc_scores[col], nbinsx=30, name=col) for col in precision_scores if col != \"best\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = precision_scores[\"best\"].value_counts().index[0]\n",
    "baseline_model_name = \"linear_svc\"\n",
    "comparison_summary = model_comparator.compare_score_to_baseline(precision_scores, baseline_model_name)\n",
    "\n",
    "percentage_time_better = round(100*(comparison_summary.loc[\"ratio_of_wins\", best_model]))\n",
    "mean_improvement = round(100*(comparison_summary.loc[\"change_in_score\", best_model]))\n",
    "print(f\"Model {best_model} is better than {baseline_model_name} {percentage_time_better}% of the time, changing precision by on {mean_improvement}% average \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd4100",
   "metadata": {},
   "source": [
    "It seems the best performing model is the tuned_lgbm model, which we will explore further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9797d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(tuned_lgbm, open(\"inference/model/tuned_lgbm.pickle\", \"wb\"))\n",
    "scores = metrics.get_scores(X_test, tuned_lgbm)\n",
    "threshold, _ = metrics.get_threshold_and_precision_at_recall(y_test, scores)\n",
    "print(f\"Threshold for model is {round(threshold,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42e286",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1209dc7",
   "metadata": {},
   "source": [
    "We will associate a status to each prediction showing whether it is a True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a190cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_status = metrics.get_confusion_status(X_test, y_test, tuned_lgbm)\n",
    "plotting_utils.plot_confusion_matrix(X_test, y_test, tuned_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe0adb",
   "metadata": {},
   "source": [
    "We want to explore visually if we can separate false positive from true positive. For that, we reduce the dimensionality to 2 using PCA and sample 450 points of each TP and FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583eb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "df_test_positive = df_test_status.loc[df_test_status['status'].isin([\"TP\", \"FP\"])]\n",
    "df_test_sample = df_test_positive.groupby(\"status\").sample(450, random_state=1)\n",
    "dimred_pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"dimred\", PCA(2))])\n",
    "X_test_scaled = dimred_pipeline.fit_transform(df_test_sample[X_test.columns])\n",
    "status = df_test_sample[\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "x, y = X_test_scaled.T\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    mode='markers',\n",
    "    hovertext=status,\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color=LabelEncoder().fit_transform(status), \n",
    "        colorscale='Viridis', \n",
    "        opacity=0.4\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d091fae",
   "metadata": {},
   "source": [
    "There doesn't seem to be any region where a status dominates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not precomputed:\n",
    "    sns_error_plot = sns.pairplot(df_test_sample[tuned_lgbm.feature_name_+['status']], hue=\"status\", plot_kws={'alpha': 0.3})\n",
    "    sns_error_plot.figure.savefig(\"error_pairplot.jpg\")\n",
    "Image.open(\"error_pairplot.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25577468",
   "metadata": {},
   "source": [
    "There are no plots where we can clearly distinguish between TP and FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c6bfe",
   "metadata": {},
   "source": [
    "### Explore extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dde13",
   "metadata": {},
   "source": [
    "Since Balance and NumOfProducts seem to be important features, a natural feature we could add is the average balance per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = preprocessing.DivideColumns(\"Balance\", \"NumOfProducts\")\n",
    "df_extra = transformer.fit_transform(df_train)\n",
    "do_tuning = False\n",
    "if do_tuning:\n",
    "    lgbm_tuner = LGBMHyperParameterTuner()\n",
    "    tuned_lgbm_extra = lgbm_tuner.get_tuned_model(df_extra, n_trials=100)\n",
    "else:\n",
    "    tuned_lgbm_extra = pickle.load(open(\"inference/model/tuned_lgbm_extra.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03819da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_feature_lgbm = Pipeline([(\"transformer\",transformer), (\"clf\", tuned_lgbm_extra) ])\n",
    "extra_feature_lgbm.fit(X_train, y_train)\n",
    "metrics.get_precision_and_roc_auc(X_test, y_test, extra_feature_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5364ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparator = ModelComparator({\"tuned_lgbm\":tuned_lgbm,\"extra_feature_lgbm\":extra_feature_lgbm} , n_tries=100)\n",
    "precision_scores, auc_scores = model_comparator.compare_models(*preprocessing.split_label(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_summary = model_comparator.compare_score_to_baseline(precision_scores, \"tuned_lgbm\")\n",
    "\n",
    "percentage_time_better = round(100*(comparison_summary.loc[\"ratio_of_wins\", \"extra_feature_lgbm\"]))\n",
    "mean_improvement = round(100*(comparison_summary.loc[\"change_in_score\", \"extra_feature_lgbm\"]))\n",
    "print(f\"Model extra_feature_lgbm is better than tuned_lgbm {percentage_time_better}% of the time, changing precision by on {mean_improvement}% average \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cdf37",
   "metadata": {},
   "source": [
    "It seems to be performing marginally worse. To see if the extra feature we added had an impact (and if that impact was the one we wanted) we can turn to SHAP values. Essentially, shap values tell us how much has each feature contributed to the prediction compared to the average prediction. Without going into the details, this is roughly done by replacing a given feature value by a random value and seeing how much it changes the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61768b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "model = extra_feature_lgbm.named_steps.clf\n",
    "explainer = shap.TreeExplainer(model)\n",
    "expected_value = explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db62da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extra_feature_lgbm.named_steps.transformer.transform(df_test_status)[model.feature_name_]\n",
    "\n",
    "explainer = shap.Explainer(model, features)\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, list):\n",
    "    expected_value = expected_value[1]\n",
    "print(f\"Explainer expected value: {expected_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0bd8f",
   "metadata": {},
   "source": [
    "Let's look at the false positives, as they are the ones we want to reduce (since we fixed the recall at 90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_given_status = df_test_status.loc[lambda df: df[\"status\"]==\"FP\"]\n",
    "# We sort by descending proba so that the first row is the hardest false positive (the one with the lowest score)\n",
    "df_given_status = df_given_status.sort_values(\"proba\",ascending=False)\n",
    "features = features.loc[df_given_status.index]\n",
    "shap_values = explainer(features)\n",
    "shap_interaction_values = explainer.shap_interaction_values(features)\n",
    "if isinstance(shap_interaction_values, list):\n",
    "    shap_interaction_values = shap_interaction_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1aa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec13b1",
   "metadata": {},
   "source": [
    "We can see that indeed the extra features (Balance/NumOfProducts) pulls the prediction to lower values, but not enough to go below the threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn",
   "language": "python",
   "name": "churn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
